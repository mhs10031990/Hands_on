{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b26659e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/home/mosaic-ai/.cache/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "\u001b[33mWARNING: The directory '/home/mosaic-ai/.cache/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "\u001b[33m  WARNING: Building wheel for shutils failed: [Errno 13] Permission denied: '/home/mosaic-ai/.cache/pip/wheels'\u001b[0m\n",
      "\u001b[31mERROR: snowflake-snowpark-python 1.0.0 has requirement snowflake-connector-python<3.0.0,>=2.7.12, but you'll have snowflake-connector-python 3.6.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: snowflake-ml-python 1.0.1 has requirement packaging<24,>=20.9, but you'll have packaging 24.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: snowflake-ml-python 1.0.1 has requirement pandas<2,>=1.0.0, but you'll have pandas 2.0.3 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: snowflake-ml-python 1.0.1 has requirement snowflake-snowpark-python<2,>=1.4.0, but you'll have snowflake-snowpark-python 1.0.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: mosaic-ai-client 1.0.0 has requirement matplotlib==3.1.1, but you'll have matplotlib 3.6.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: jupyterlab-server 2.25.4 has requirement jsonschema>=4.18.0, but you'll have jsonschema 3.2.0 which is incompatible.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/cloudpickle already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/cloudpickle-1.6.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/backports already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/configparser-7.0.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/pymysql already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/PyMySQL-1.1.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/shutils already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/shutils-0.1.0-py3.8.egg-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/urllib3 already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/urllib3-1.26.15.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/PyYAML-6.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/_yaml already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/yaml already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/charset_normalizer already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/charset_normalizer-3.3.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/certifi already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/certifi-2024.2.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/idna already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/idna-3.7.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/requests already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/requests-2.31.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/requests_toolbelt already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/requests_toolbelt-0.9.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/threadpoolctl.py already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/threadpoolctl-3.5.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/__pycache__ already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/joblib already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/joblib-1.4.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/numpy.libs already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/numpy-1.24.4.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/numpy already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/scipy already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/scipy.libs already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/scipy-1.10.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/sklearn already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/scikit_learn.libs already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/scikit_learn-1.2.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/mosaic_utils already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/mosaic_utils-1.0.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/refractml already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/tests already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/refractml-1.0.4.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/bin already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --q refractml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1afd8123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/home/mosaic-ai/.cache/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "\u001b[33mWARNING: The directory '/home/mosaic-ai/.cache/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "\u001b[31mERROR: snowflake-ml-python 1.0.1 has requirement packaging<24,>=20.9, but you'll have packaging 24.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: snowflake-ml-python 1.0.1 has requirement pandas<2,>=1.0.0, but you'll have pandas 2.0.3 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: snowflake-ml-python 1.0.1 has requirement snowflake-snowpark-python<2,>=1.4.0, but you'll have snowflake-snowpark-python 1.0.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: snowflake-ml-python 1.0.1 has requirement xgboost<2,>=1.7.3, but you'll have xgboost 2.0.3 which is incompatible.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/numpy.libs already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/numpy-1.24.4.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/numpy already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/scipy already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/scipy.libs already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/scipy-1.10.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/bin already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --q xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08b4f5f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/home/mosaic-ai/.cache/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "\u001b[33mWARNING: The directory '/home/mosaic-ai/.cache/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "\u001b[31mERROR: snowflake-ml-python 1.0.1 has requirement packaging<24,>=20.9, but you'll have packaging 24.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: snowflake-ml-python 1.0.1 has requirement pandas<2,>=1.0.0, but you'll have pandas 2.0.3 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: snowflake-ml-python 1.0.1 has requirement snowflake-snowpark-python<2,>=1.4.0, but you'll have snowflake-snowpark-python 1.0.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: snowflake-ml-python 1.0.1 has requirement xgboost<2,>=1.7.3, but you'll have xgboost 2.0.3 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: openapi-spec-validator 0.7.1 has requirement jsonschema<5.0.0,>=4.18.0, but you'll have jsonschema 3.2.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: openapi-schema-validator 0.6.2 has requirement jsonschema<5.0.0,>=4.19.1, but you'll have jsonschema 3.2.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: mosaic-ai-serving 1.0.0 has requirement Flask==2.1.1; python_version >= \"3.7\", but you'll have flask 2.2.5 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: mosaic-ai-serving 1.0.0 has requirement matplotlib==3.6.0; python_version >= \"3.8\", but you'll have matplotlib 3.7.5 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: mosaic-ai-client 1.0.0 has requirement matplotlib==3.1.1, but you'll have matplotlib 3.7.5 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: jupyterlab 3.2.4 has requirement jupyter-server~=1.4, but you'll have jupyter-server 2.0.0a1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: jupyterlab-server 2.25.4 has requirement jsonschema>=4.18.0, but you'll have jsonschema 3.2.0 which is incompatible.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/numpy.libs already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/numpy-1.24.4.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/numpy already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/__pycache__ already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/scipy already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/scipy.libs already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/scipy-1.10.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/bin already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --q catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8bd62d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark import Session\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32837b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import skew\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error, recall_score, precision_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from math import sqrt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "from catboost import CatBoostClassifier\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1efa78b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "from refractml import *\n",
    "from refractml.constants import MLModelFlavours\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8078d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "NFOLDS = 3\n",
    "SEED = 0\n",
    "NROWS = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22ce04b",
   "metadata": {},
   "source": [
    "# Code to establish connection and read data from Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c256fe2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['snowflake_connection.ini']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read(\"snowflake_connection.ini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4864e1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_parameters = {\n",
    "    \"user\": f'{config[\"Snowflake\"][\"user\"]}',\n",
    "    \"password\": f'{config[\"Snowflake\"][\"password\"]}',\n",
    "    #\"password\": os.getenv('snowflake_password'),\n",
    "    \"account\": f'{config[\"Snowflake\"][\"account\"]}',\n",
    "    #\"account\": os.getenv('snowflake_account'),\n",
    "    \"WAREHOUSE\": f'{config[\"Snowflake\"][\"WAREHOUSE\"]}',\n",
    "    \"DATABASE\": f'{config[\"Snowflake\"][\"DATABASE\"]}',\n",
    "    \"SCHEMA\": f'{config[\"Snowflake\"][\"SCHEMA\"]}'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94d0200d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connection successful!\n"
     ]
    }
   ],
   "source": [
    "def snowflake_connector(conn):\n",
    "    try:\n",
    "        session = Session.builder.configs(conn).create()\n",
    "        print(\"connection successful!\")\n",
    "    except:\n",
    "        raise ValueError(\"error while connecting with db\")\n",
    "    return session\n",
    "\n",
    "session = snowflake_connector(connection_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ebc2b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "application_train_sf  = session.table(\"CRA_APPLICATION_TRAIN_DETAILS\")\n",
    "application_test_sf  = session.table(\"CRA_APPLICATION_TEST_DETAILS\")\n",
    "previous_application_sf  = session.table(\"CRA_PREVIOUS_APPLICATION_DETAILS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0b4b30",
   "metadata": {},
   "source": [
    "# Convert Snowflake data into Pandas dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b65c096",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = application_train_sf.to_pandas()\n",
    "test = application_test_sf.to_pandas()\n",
    "prev = previous_application_sf.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07ded794",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['CREATED_BY','CREATED_AT'], axis=1, inplace=True)\n",
    "test.drop(['CREATED_BY','CREATED_AT'], axis=1, inplace=True)\n",
    "prev.drop(['CREATED_BY','CREATED_AT'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f1a2832",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "categorical_feats = [\n",
    "    f for f in data.columns if data[f].dtype == 'object'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9f1651c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f_ in categorical_feats:\n",
    "    data[f_], indexer = pd.factorize(data[f_])\n",
    "    test[f_] = indexer.get_indexer(test[f_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7d8cf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.enable()\n",
    "\n",
    "y_train = data['TARGET']\n",
    "del data['TARGET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a77babae",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_cat_features = [\n",
    "    f_ for f_ in prev.columns if prev[f_].dtype == 'object'\n",
    "]\n",
    "for f_ in prev_cat_features:\n",
    "    prev[f_], _ = pd.factorize(prev[f_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f752203",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_prev = prev.groupby('SK_ID_CURR').mean()\n",
    "cnt_prev = prev[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\n",
    "avg_prev['nb_app'] = cnt_prev['SK_ID_PREV']\n",
    "del avg_prev['SK_ID_PREV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87527ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data.merge(right=avg_prev.reset_index(), how='left', on='SK_ID_CURR')\n",
    "x_test = test.merge(right=avg_prev.reset_index(), how='left', on='SK_ID_CURR')\n",
    "\n",
    "x_train = x_train.fillna(0)\n",
    "x_test= x_test.fillna(0)\n",
    "\n",
    "ntrain = x_train.shape[0]\n",
    "ntest = x_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93e01200",
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_feats = ['SK_ID_CURR']\n",
    "features = [f_ for f_ in x_train.columns if f_ not in excluded_feats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d847db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train[features]\n",
    "x_test = x_test[features]\n",
    "\n",
    "kf = KFold(n_splits = NFOLDS, shuffle=True, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ab19ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SklearnWrapper(object):\n",
    "    def __init__(self, clf, seed=0, params=None):\n",
    "        params['random_state'] = seed\n",
    "        self.clf = clf(**params)\n",
    "\n",
    "    def train(self, x_train, y_train):\n",
    "        self.clf.fit(x_train, y_train)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.clf.predict_proba(x)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a44758c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CatboostWrapper(object):\n",
    "    def __init__(self, clf, seed=0, params=None):\n",
    "        params['random_seed'] = seed\n",
    "        self.clf = clf(**params)\n",
    "\n",
    "    def train(self, x_train, y_train):\n",
    "        self.clf.fit(x_train, y_train)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.clf.predict_proba(x)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a3a5d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XgbWrapper(object):\n",
    "    def __init__(self, seed=0, params=None):\n",
    "        self.param = params\n",
    "        self.param['seed'] = seed\n",
    "        self.nrounds = params.pop('nrounds', 250)\n",
    "\n",
    "    def train(self, x_train, y_train):\n",
    "        dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "        self.gbdt = xgb.train(self.param, dtrain, self.nrounds)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.gbdt.predict(xgb.DMatrix(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "604c71ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_oof(clf):\n",
    "    oof_train = np.zeros((ntrain,))\n",
    "    oof_test = np.zeros((ntest,))\n",
    "    oof_test_skf = np.empty((NFOLDS, ntest))\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(x_train)):\n",
    "        x_tr = x_train.loc[train_index]\n",
    "        y_tr = y_train.loc[train_index]\n",
    "        x_te = x_train.loc[test_index]\n",
    "\n",
    "        clf.train(x_tr, y_tr)\n",
    "\n",
    "        oof_train[test_index] = clf.predict(x_te)\n",
    "        oof_test_skf[i, :] = clf.predict(x_test)\n",
    "\n",
    "    oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ff5f6e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "et_params = {\n",
    "    'n_jobs': 16,\n",
    "    'n_estimators': 200,\n",
    "    'max_features': 0.5,\n",
    "    'max_depth': 12,\n",
    "    'min_samples_leaf': 2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ee53240",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_params = {\n",
    "    'n_jobs': 16,\n",
    "    'n_estimators': 200,\n",
    "    'max_features': 0.2,\n",
    "    'max_depth': 12,\n",
    "    'min_samples_leaf': 2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "78415a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "    'seed': 0,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'silent': 1,\n",
    "    'subsample': 0.7,\n",
    "    'learning_rate': 0.075,\n",
    "    'objective': 'binary:logistic',\n",
    "    'max_depth': 4,\n",
    "    'num_parallel_tree': 1,\n",
    "    'min_child_weight': 1,\n",
    "    'nrounds': 200\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "14272538",
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost_params = {\n",
    "    'iterations': 200,\n",
    "    'learning_rate': 0.5,\n",
    "    'depth': 3,\n",
    "    'l2_leaf_reg': 40,\n",
    "    'bootstrap_type': 'Bernoulli',\n",
    "    'subsample': 0.7,\n",
    "    'scale_pos_weight': 5,\n",
    "    'eval_metric': 'AUC',\n",
    "    'od_type': 'Iter',\n",
    "    'allow_writing_files': False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cb755482",
   "metadata": {},
   "outputs": [],
   "source": [
    "xg = XgbWrapper(seed=SEED, params=xgb_params)\n",
    "et = SklearnWrapper(clf=ExtraTreesClassifier, seed=SEED, params=et_params)\n",
    "rf = SklearnWrapper(clf=RandomForestClassifier, seed=SEED, params=rf_params)\n",
    "cb = CatboostWrapper(clf= CatBoostClassifier, seed = SEED, params=catboost_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35afc22b",
   "metadata": {},
   "source": [
    "# XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093fa990",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/pip_packages/xgboost/core.py:160: UserWarning: [09:25:04] WARNING: /workspace/src/learner.cc:742: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "xg_oof_train, xg_oof_test = get_oof(xg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c152d938",
   "metadata": {},
   "outputs": [],
   "source": [
    "@scoring_func\n",
    "def score(model, request):\n",
    "    payload_dict = request.json[\"payload\"]\n",
    "    data = pd.DataFrame(payload_dict,index=[0])\n",
    "    y_pred = model.predict(model)\n",
    "    y_prob = model.predict_proba(data)[:,1]\n",
    "    temp_dict = {\"Prediction: \": y_pred, \"Probability: \": y_prod }\n",
    "    return temp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ec086d",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload  = x_train.iloc[0].to_dict()\n",
    "payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3765ef37",
   "metadata": {},
   "outputs": [],
   "source": [
    "req = requests.Request()\n",
    "req.json = {\"payload\":payload}\n",
    "y_req = req\n",
    "output = score(xg, y_req)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8ece8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## registering the model in refract.\n",
    "model_reg = register_model(model, \n",
    "               score, \n",
    "               name=\"CRA_XGB_Model\", \n",
    "               description=\"Credit Risk XGBoost Model\",\n",
    "               flavour=MLModelFlavours.sklearn,\n",
    "               model_type=\"classification\",\n",
    "               init_script=\"pip install snowflake-ml-python==1.0.11\",\n",
    "               y_true=y_test,\n",
    "               y_pred=y_pred, \n",
    "               features=X_train.columns,\n",
    "               input_type=\"json\", \n",
    "               explain_ai=True,\n",
    "               x_train=X_train, \n",
    "               x_test=X_test, \n",
    "               y_train=y_train.tolist(),\n",
    "               y_test=y_test.tolist(),\n",
    "               feature_names=X_train.columns.tolist(),\n",
    "               original_features=X_train.columns.tolist(),\n",
    "               feature_ids=X_train.columns,\n",
    "               kyd=True, kyd_score = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "015e2dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "et_oof_train, et_oof_test = get_oof(et)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49af35fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_oof_train, rf_oof_test = get_oof(rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24749c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_oof_train, cb_oof_test = get_oof(cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50423e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@scoring_func\n",
    "def score(model, request):\n",
    "    payload_dict = request.json[\"payload\"]\n",
    "    data = pd.DataFrame(payload_dict,index=[0])\n",
    "    y_pred = model.predict(model)\n",
    "    y_prob = model.predict_proba(data)[:,1]\n",
    "    temp_dict = {\"Prediction: \": y_pred, \"Probability: \": y_prod }\n",
    "    return temp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de4e72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload  = x_train.iloc[0].to_dict()\n",
    "payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bce238",
   "metadata": {},
   "outputs": [],
   "source": [
    "req = requests.Request()\n",
    "req.json = {\"payload\":payload}\n",
    "y_req = req\n",
    "output = score(xg, y_req)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7648ac73",
   "metadata": {},
   "outputs": [],
   "source": [
    "## registering the model in refract.\n",
    "model_reg = register_model(model, \n",
    "               score, \n",
    "               name=\"CRA_XGB_Model\", \n",
    "               description=\"Credit Risk XGBoost Model\",\n",
    "               flavour=MLModelFlavours.sklearn,\n",
    "               model_type=\"classification\",\n",
    "               init_script=\"pip install snowflake-ml-python==1.0.11\",\n",
    "               y_true=y_test,\n",
    "               y_pred=y_pred, \n",
    "               features=X_train.columns,\n",
    "               input_type=\"json\", \n",
    "               explain_ai=True,\n",
    "               x_train=X_train, \n",
    "               x_test=X_test, \n",
    "               y_train=y_train.tolist(),\n",
    "               y_test=y_test.tolist(),\n",
    "               feature_names=X_train.columns.tolist(),\n",
    "               original_features=X_train.columns.tolist(),\n",
    "               feature_ids=X_train.columns,\n",
    "               kyd=True, kyd_score = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ed7c6db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost-CV Recall-Score: 0.020503524672708964\n",
      "Extra Tree-CV Recall-Score: 0.00016112789526686808\n",
      "RandomForest-CV Recall-Score: 0.0002014098690835851\n",
      "CatBoost-CV Recall-Score: 0.38328298086606244\n"
     ]
    }
   ],
   "source": [
    "print(\"XGBoost-CV Recall-Score: {}\".format((recall_score(y_train, np.round(xg_oof_train)))))\n",
    "print(\"Extra Tree-CV Recall-Score: {}\".format((recall_score(y_train, np.round(et_oof_train)))))\n",
    "print(\"RandomForest-CV Recall-Score: {}\".format((recall_score(y_train, np.round(rf_oof_train)))))\n",
    "print(\"CatBoost-CV Recall-Score: {}\".format((recall_score(y_train, np.round(cb_oof_train)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "21d1073a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost-CV Precision-Score: 0.5520607375271149\n",
      "Extra Tree-CV Precision-Score: 0.8\n",
      "RandomForest-CV Precision-Score: 0.8333333333333334\n",
      "CatBoost-CV Precision-Score: 0.2610568481123793\n"
     ]
    }
   ],
   "source": [
    "print(\"XGBoost-CV Precision-Score: {}\".format((precision_score(y_train, np.round(xg_oof_train)))))\n",
    "print(\"Extra Tree-CV Precision-Score: {}\".format((precision_score(y_train, np.round(et_oof_train)))))\n",
    "print(\"RandomForest-CV Precision-Score: {}\".format((precision_score(y_train, np.round(rf_oof_train)))))\n",
    "print(\"CatBoost-CV Precision-Score: {}\".format((precision_score(y_train, np.round(cb_oof_train)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "fbd92ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost-CV F1-Score: 0.03953858701984697\n",
      "Extra Tree-CV F1-Score: 0.0003221908981071284\n",
      "RandomForest-CV F1-Score: 0.0004027224034473038\n",
      "CatBoost-CV F1-Score: 0.31057725262350466\n"
     ]
    }
   ],
   "source": [
    "print(\"XGBoost-CV F1-Score: {}\".format((f1_score(y_train, np.round(xg_oof_train)))))\n",
    "print(\"Extra Tree-CV F1-Score: {}\".format((f1_score(y_train, np.round(et_oof_train)))))\n",
    "print(\"RandomForest-CV F1-Score: {}\".format((f1_score(y_train, np.round(rf_oof_train)))))\n",
    "print(\"CatBoost-CV F1-Score: {}\".format((f1_score(y_train, np.round(cb_oof_train)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "598b0958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost-CV AUC-Score: 0.5095212698464541\n",
      "Extra Tree-CV AUC-Score: 0.5000787952006845\n",
      "RandomForest-CV AUC-Score: 0.5000989361875929\n",
      "CatBoost-CV AUC-Score: 0.6440038288579975\n"
     ]
    }
   ],
   "source": [
    "print(\"XGBoost-CV AUC-Score: {}\".format((roc_auc_score(y_train, np.round(xg_oof_train)))))\n",
    "print(\"Extra Tree-CV AUC-Score: {}\".format((roc_auc_score(y_train, np.round(et_oof_train)))))\n",
    "print(\"RandomForest-CV AUC-Score: {}\".format((roc_auc_score(y_train, np.round(rf_oof_train)))))\n",
    "print(\"CatBoost-CV AUC-Score: {}\".format((roc_auc_score(y_train, np.round(cb_oof_train)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "29974030",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.concatenate((xg_oof_train, et_oof_train, rf_oof_train, cb_oof_train), axis=1)\n",
    "x_test = np.concatenate((xg_oof_test, et_oof_test, rf_oof_test, cb_oof_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "aebcc721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(307511, 4),(48744, 4)\n"
     ]
    }
   ],
   "source": [
    "print(\"{},{}\".format(x_train.shape, x_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ac9a9b67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_regression = RandomForestClassifier()\n",
    "logistic_regression.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "6f3aa01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_log_train = logistic_regression.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "2bacd8eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24825"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "88f62f18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24803"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_log_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "5a6bdafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest-Stacking Recall-Score: 0.9991137965760323\n",
      "RandomForest-Stacking Precision-Score: 1.0\n",
      "RandomForest-Stacking F1-Score: 0.9995567018618523\n",
      "RandomForest-Stacking AUC-Score: 0.9995568982880161\n"
     ]
    }
   ],
   "source": [
    "print(\"RandomForest-Stacking Recall-Score: {}\".format((recall_score(y_train, y_log_train))))\n",
    "print(\"RandomForest-Stacking Precision-Score: {}\".format((precision_score(y_train, y_log_train))))\n",
    "print(\"RandomForest-Stacking F1-Score: {}\".format((f1_score(y_train, y_log_train))))\n",
    "print(\"RandomForest-Stacking AUC-Score: {}\".format((roc_auc_score(y_train, y_log_train))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "b54c0f98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest-Stacking AUC-Score: [[282686      0]\n",
      " [    22  24803]]\n"
     ]
    }
   ],
   "source": [
    "print(\"RandomForest-Stacking AUC-Score: {}\".format((confusion_matrix(y_train, y_log_train))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fbcb01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
