{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937a24d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --q seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd8ce05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install --q xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315718f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "from numpy.random import randint\n",
    "from multiprocessing import cpu_count\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from snowflake.snowpark import Session\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8575152",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression ,SGDClassifier\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline #pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import RFE #for feature selection\n",
    "#from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import MinMaxScaler,LabelEncoder\n",
    "from sklearn.metrics import (f1_score,classification_report,\n",
    "                             roc_auc_score,confusion_matrix,roc_curve,auc) # evaluatin metrics\n",
    "from sklearn.model_selection import KFold,RandomizedSearchCV ,cross_val_score,RepeatedStratifiedKFold\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier ,RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2755bbd8",
   "metadata": {},
   "source": [
    "### Code to establish connection and read data from Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0cc9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read(\"snowflake_connection.ini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39954ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_parameters = {\n",
    "    \"user\": f'{config[\"Snowflake\"][\"user\"]}',\n",
    "    \"password\": f'{config[\"Snowflake\"][\"password\"]}',\n",
    "    #\"password\": os.getenv('snowflake_password'),\n",
    "    \"account\": f'{config[\"Snowflake\"][\"account\"]}',\n",
    "    #\"account\": os.getenv('snowflake_account'),\n",
    "    \"WAREHOUSE\": f'{config[\"Snowflake\"][\"WAREHOUSE\"]}',\n",
    "    \"DATABASE\": f'{config[\"Snowflake\"][\"DATABASE\"]}',\n",
    "    \"SCHEMA\": f'{config[\"Snowflake\"][\"SCHEMA\"]}'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f2291b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def snowflake_connector(conn):\n",
    "    try:\n",
    "        session = Session.builder.configs(conn).create()\n",
    "        print(\"connection successful!\")\n",
    "    except:\n",
    "        raise ValueError(\"error while connecting with db\")\n",
    "    return session\n",
    "\n",
    "session = snowflake_connector(connection_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f779c97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "application_train_sf  = session.table(\"CRA_APPLICATION_TRAIN_DETAILS\")\n",
    "application_test_sf  = session.table(\"CRA_APPLICATION_TEST_DETAILS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7c257c",
   "metadata": {},
   "outputs": [],
   "source": [
    "application_train = application_train_sf.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0678e03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "application_test = application_test_sf.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9a82ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 30)\n",
    "def info(table):\n",
    "    print(f'\\n the shape of the table is : \\n',table.shape)\n",
    "    print('-' * 80)\n",
    "    print('\\n data types : ', table.dtypes.value_counts().sort_values())\n",
    "    print('-' * 80)\n",
    "    missings=table.isna().mean()[table.isna().mean().sort_values(ascending=False)!=0].sort_values(ascending=False)\n",
    "    print('-' * 80)\n",
    "    print('\\n number of features having missing data : ',len(missings))\n",
    "    print('-' * 80)\n",
    "    print('\\n missing data in the table: \\n',missings)\n",
    "    print('-' * 80)\n",
    "    print(missings[missings>0.5])\n",
    "    print('-' * 80)\n",
    "    print('\\n missing data over 50% : ',len(missings[missings>0.5]))\n",
    "info(application_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1650d549",
   "metadata": {},
   "source": [
    "The data we are working with contains307511 observations and 122 columns only 16 of them are categorical and the rest are numeric .\n",
    "But the problem we are facing here is the huge amout of missing data with 67 columns contains missing informations with proportion more than 60% exceed threshold of 50% of missingness .\n",
    "To tackle this problem we should do further analysis and select best strategies to handle them ,so let's start with the analysis first and keep this task to the processing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b62221",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = application_train.TARGET.value_counts()\n",
    "df1 = pd.DataFrame({'labels': temp.index,\n",
    "                   'values': temp.values/len(application_train)})\n",
    "plt.figure(figsize = (4,4))\n",
    "plt.title('defaut')\n",
    "sns.set_color_codes(\"pastel\")\n",
    "sns.barplot(x = 'labels', y=\"values\", data=df1)\n",
    "locs, labels = plt.xticks()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e880aca",
   "metadata": {},
   "source": [
    "From the distribution plot of the TARGET column we notice than our data suffers from imbalanced classes , There are far more loans that were repaid on time than loans that were not repaid.This may affect the model and results in biased predictions toward the non default class.It's important to choose the right option to handle this issue before running model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a523d723",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMERIC_FEATURES = application_train.select_dtypes(['int8','int16','float16','int32','float32','int64','float64']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b694b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "application_train[NUMERIC_FEATURES].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed11291",
   "metadata": {},
   "source": [
    "From this dataframe numeric features are not well identified and contains categorical features representent as numerical ones and these are :\n",
    "\n",
    "SK_ID_CURR is an index not feature.\n",
    "TARGET ,children represents the number of children the applicant have.\n",
    "FLAG_DOCUMENT_2, FLAG_DOCUMENT_3, .... FLAG_DOCUMENT_20 ,FLAG_DOCUMENT_21 :These features are binary flags indicating the presence or absence of specific documents in the applicant's file. Each flag represents a different type of document (e.g., identification documents, income documents, etc.)\n",
    "\n",
    "another finding is that the group :\n",
    "AMT_REQ_CREDIT_BUREAU_HOUR,AMT_REQ_CREDIT_BUREAU_DAY,AMT_REQ_CREDIT_BUREAU_WEEK,AMT_REQ_CREDIT_BUREAU_MON, AMT_REQ_CREDIT_BUREAU_QRT,AMT_REQ_CREDIT_BUREAU_YEAR represent the number of inquiries or requests made to the Credit Bureau by the applicant within specific time intervals. Each feature corresponds to a different time unit (hour, day, week, month, quarter, year)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9c4b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXT_SOURCE_TARGET=['EXT_SOURCE_1','EXT_SOURCE_2', 'EXT_SOURCE_3','TARGET']\n",
    "application_train[EXT_SOURCE_TARGET].corr().style.format(\"{:.4}\").background_gradient(cmap=plt.get_cmap('coolwarm'), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746964cc",
   "metadata": {},
   "source": [
    "EXT_SOURCE features have small correlation between each other and they have small negative correlatons with the target ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221d9d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_test(df):\n",
    "    t_stat_list=[]\n",
    "    p_val_list=[]\n",
    "    variable=[]\n",
    "\n",
    "    for var in df.columns:    \n",
    "        t_stat, p_val = stats.ttest_ind(application_train[application_train['TARGET'] == 0][var],\n",
    "                                        application_train[application_train['TARGET'] == 1][var])\n",
    "        if p_val < 0.05:\n",
    "            variable.append(var)\n",
    "            t_stat_list.append(t_stat)\n",
    "            p_val_list.append(p_val)\n",
    "\n",
    "    t_test = pd.DataFrame({'variable': variable, 't_stat': t_stat_list, 'p_val': p_val_list})\n",
    "    return t_test.sort_values(by='p_val',ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7a0a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_test(application_train.select_dtypes(['int8','int16','float16','int32','float32','int64','float64']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7525a65c",
   "metadata": {},
   "source": [
    "DAYS_BIRTH,REGION_RATING_CLIENT_W_CITY,REGION_RATING_CLIENT, DAYS_ID_PUBLISH REGION_POPULATION_RELATIVE, LIVE_CITY_NOT_WORK_CITY,AMT_CREDIT, FLAG_DOCUMENT_6, FLAG_WORK_PHONE, HOUR_APPR_PROCESS_START, FLAG_PHONE, CNT_CHILDREN, FLAG_DOCUMENT_16, FLAG_DOCUMENT_13, FLAG_DOCUMENT_14, FLAG_DOCUMENT_8, FLAG_DOCUMENT_18, REG_REGION_NOT_WORK_REGION, FLAG_DOCUMENT_15, REG_REGION_NOT_LIVE_REGION, FLAG_DOCUMENT_2, FLAG_DOCUMENT_9, FLAG_DOCUMENT_11, AMT_INCOME_TOTAL, FLAG_DOCUMENT_21\n",
    "These features have been identified as important based on the specific analysis performed( ttest), considering their potential relevance to the credit default prediction. However, it's essential to note that the importance of features can vary depending on the context, and modeling approach. Further investigation and modeling can help validate their significance and contribution to the predictive power of the models,so we will keep all the features for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5020ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "application_train.select_dtypes('O').nunique().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ef478a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_stats(df, FEATURES):\n",
    "    for feature in FEATURES:\n",
    "        temp = df[feature].value_counts()\n",
    "        df1 = pd.DataFrame({feature: temp.index, 'value': temp.values})\n",
    "        cat_perc_0 = df[df['TARGET'] == 0].groupby(feature).size().reset_index(name='Count_Target_0')\n",
    "        cat_perc_1 = df[df['TARGET'] == 1].groupby(feature).size().reset_index(name='Count_Target_1')\n",
    "        cat_perc = cat_perc_0.merge(cat_perc_1, how='left', on=feature).fillna(0)\n",
    "        cat_perc['Percentage_Target_0'] = cat_perc['Count_Target_0'] / (cat_perc['Count_Target_0'] + cat_perc['Count_Target_1']) * 100\n",
    "        cat_perc['Percentage_Target_1'] = cat_perc['Count_Target_1'] / (cat_perc['Count_Target_0'] + cat_perc['Count_Target_1']) * 100\n",
    "        cat_perc.sort_values(by=feature, inplace=True)\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 6))\n",
    "        \n",
    "        sns.set_color_codes(\"pastel\")\n",
    "        \n",
    "        # Plot distribution for TARGET == 0\n",
    "        sns.barplot(ax=ax1, x=feature, y=\"Percentage_Target_0\", data=cat_perc)\n",
    "        ax1.set_xticklabels(ax1.get_xticklabels(), rotation=90)\n",
    "        \n",
    "        # Plot distribution for TARGET == 1\n",
    "        sns.barplot(ax=ax2, x=feature, y='Percentage_Target_1', data=cat_perc)\n",
    "        ax2.set_xticklabels(ax2.get_xticklabels(), rotation=90)\n",
    "        \n",
    "        \n",
    "        plt.tick_params(axis='both', which='major', labelsize=10)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0bebe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "important_features=['CODE_GENDER','NAME_EDUCATION_TYPE','NAME_FAMILY_STATUS','OCCUPATION_TYPE',\n",
    "'ORGANIZATION_TYPE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dafd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_stats(application_train,important_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c7baef",
   "metadata": {},
   "source": [
    "for the first question, it seems that women are less defaulted than men and they are more repayable than men.So as a recommandation to the bank is to improve the procedure of repayment of loans given to men.\n",
    "\n",
    "for the second question, poeple with a low education level shows the most defaulters ,in the first place people with less than secondary school. Also people with a hight education level are less prone to default. This feature show a significative difference of the distributions of defaulter and non defaulter so it will be helpfull for the modeling task.\n",
    "\n",
    "for the third question, widow category are the least defaulder and the civil marriage category are the most defaulter category ,and this can be explained with the high amout of expences for families so they maight face problems in paying their annuity.\n",
    "\n",
    "for the forth question ,the most likely category that defaults is the low labores and the less category to default is accountant ,this is very logic .Another finding from this plot is that the all categories have uniform destribution in the non defaulter set ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3bb754",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce09f94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform chi-squared test\n",
    "def chi2_test(df):\n",
    "    chi2_stat_list=[]; p_val_list=[]; dof_list=[]; ex_list=[]\n",
    "    for var in df.columns:\n",
    "        chi2_stat, p_val, dof, ex = stats.chi2_contingency(pd.crosstab(application_train['TARGET'], application_train[var]))\n",
    "        chi2_stat_list.append(chi2_stat); p_val_list.append(p_val); dof_list.append(dof); ex_list.append(ex);\n",
    "    chi_squared=pd.DataFrame({'variable':df.columns,'dof':dof_list,'chi2_stat':chi2_stat_list,'p_val':p_val_list})\n",
    "    chi_squared.set_index('variable',inplace=True)\n",
    "    print(chi_squared.sort_values(by='p_val',ascending=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d76f41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chi2_test(application_train.select_dtypes('O'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c720d2",
   "metadata": {},
   "source": [
    "the features that we assumed to be important are the same the most significative one .This garante our approach and strategy.\n",
    "\n",
    "More than inatial ones ,features such as NAME_HOUSING_TYPE, FLAG_OWN_CAR , NAME_CONTRACT_TYPE tends to be significatif and helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951f830f",
   "metadata": {},
   "outputs": [],
   "source": [
    "application_train.drop(['SK_ID_CURR','CREATED_BY','CREATED_AT'],inplace=True,axis=1)\n",
    "SK_ID_CURR=application_test['SK_ID_CURR']\n",
    "application_test.drop(['SK_ID_CURR','CREATED_BY','CREATED_AT'],inplace=True,axis=1)\n",
    "application_train['DAYS_EMPLOYED'].replace(365243,np.nan,inplace=True)\n",
    "application_test['DAYS_EMPLOYED'].replace(365243,np.nan,inplace=True)\n",
    "application_train['CODE_GENDER'].replace('XNA',np.nan,inplace=True)\n",
    "application_test['CODE_GENDER'].replace('XNA',np.nan,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee36053",
   "metadata": {},
   "source": [
    "Aggregating the 20 flag document features into a single feature by summing the number of flag documents per applicant can be a useful approach. By creating a new aggregated feature, we can capture the overall count or presence of flag documents for each applicant and reduce sparsity of the data, which may provide valuable information for the analysis or modeling task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbff4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAG_DOCUMENT=['FLAG_DOCUMENT_2',\n",
    "'FLAG_DOCUMENT_3', 'FLAG_DOCUMENT_4', 'FLAG_DOCUMENT_5',\n",
    "'FLAG_DOCUMENT_6', 'FLAG_DOCUMENT_7', 'FLAG_DOCUMENT_8',\n",
    "'FLAG_DOCUMENT_9', 'FLAG_DOCUMENT_10', 'FLAG_DOCUMENT_11',\n",
    "'FLAG_DOCUMENT_12', 'FLAG_DOCUMENT_13', 'FLAG_DOCUMENT_14',\n",
    "'FLAG_DOCUMENT_15', 'FLAG_DOCUMENT_16', 'FLAG_DOCUMENT_17',\n",
    "'FLAG_DOCUMENT_18', 'FLAG_DOCUMENT_19', 'FLAG_DOCUMENT_20',\n",
    "'FLAG_DOCUMENT_21']\n",
    "application_train['FLAG_DOCUMENT']=application_train[FLAG_DOCUMENT].sum(axis=1)\n",
    "application_test['FLAG_DOCUMENT']=application_test[FLAG_DOCUMENT].sum(axis=1)\n",
    "application_train.drop(FLAG_DOCUMENT,axis=1,inplace=True)\n",
    "application_test.drop(FLAG_DOCUMENT,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43259028",
   "metadata": {},
   "source": [
    "we should drop the AMT_REQ_CREDIT features because of they have no impact to the target and this may cause model missleading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e1663c",
   "metadata": {},
   "outputs": [],
   "source": [
    "AMT_REQ_CREDIT=['AMT_REQ_CREDIT_BUREAU_HOUR','AMT_REQ_CREDIT_BUREAU_DAY',\n",
    "                'AMT_REQ_CREDIT_BUREAU_WEEK','AMT_REQ_CREDIT_BUREAU_MON', 'AMT_REQ_CREDIT_BUREAU_QRT']\n",
    "application_train.drop(AMT_REQ_CREDIT,axis=1,inplace=True)\n",
    "application_test.drop(AMT_REQ_CREDIT,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9162765d",
   "metadata": {},
   "source": [
    "Two new features, AGE and YEARS_EMPLOYED, were created to provide additional information about the applicants' age and years of employment, respectively.\n",
    "also DEBT_TO_INCOME_RATIO: This feature calculates the debt-to-income ratio by dividing the applicant's total credit amount by their total income."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c823e564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age Feature\n",
    "application_train['AGE']=application_train['DAYS_BIRTH']/(-365)\n",
    "application_test['AGE']=application_test['DAYS_BIRTH']/(-365)\n",
    "# MEAN_EXT_SOURCE\n",
    "application_train['MEAN_EXT_SOURCE'] = application_train[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].mean(axis=1)\n",
    "application_test['MEAN_EXT_SOURCE'] = application_test[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].mean(axis=1)\n",
    "# Generate YEARS_EMPLOYED\n",
    "application_train['YEARS_EMPLOYED']=application_train['DAYS_EMPLOYED']/(-365)\n",
    "application_test['YEARS_EMPLOYED']=application_test['DAYS_EMPLOYED']/(-365)\n",
    "# DAYS_BIRTH_SQUARED\n",
    "application_train['DAYS_BIRTH_SQUARED'] = application_train['DAYS_BIRTH'] ** 2\n",
    "application_test['DAYS_BIRTH_SQUARED'] = application_test['DAYS_BIRTH'] ** 2\n",
    "# Generate DEBT_TO_INCOME_RATIO \n",
    "application_train['DEBT_TO_INCOME_RATIO'] = application_train['AMT_CREDIT'] / application_train['AMT_INCOME_TOTAL']\n",
    "application_test['DEBT_TO_INCOME_RATIO'] = application_test['AMT_CREDIT'] / application_test['AMT_INCOME_TOTAL']\n",
    "# Generate INCOME_TO_CREDIT \n",
    "application_train['INCOME_TO_CREDIT'] = application_train['AMT_INCOME_TOTAL'] / application_train['AMT_CREDIT']\n",
    "application_test['INCOME_TO_CREDIT'] = application_test['AMT_INCOME_TOTAL'] / application_test['AMT_CREDIT']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd696a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_numeric_features=['AGE','YEARS_EMPLOYED','DEBT_TO_INCOME_RATIO','INCOME_TO_CREDIT','DAYS_BIRTH_SQUARED']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b5600a",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_test(application_train[new_numeric_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533666e3",
   "metadata": {},
   "source": [
    "additional feature engineering was performed on both datasets. The following features were created based on patterns and analysis from EDA phase :\n",
    "\n",
    "_INCOME_CLASS: This feature categorizes the AMT_INCOME_TOTAL into three income classes: 'Low_INCOME', 'Medium_INCOME', and 'High_INCOME'.\n",
    "\n",
    "ANNUITY_CLASS: This feature divides the AMT_ANNUITY into three annuity classes: 'Low_ANNUITY', 'Medium_ANNUITY', and 'High_ANNUITY'.\n",
    "\n",
    "AMT_GOODS_PRICE_0.2_0.7: This binary feature indicates whether the AMT_GOODS_PRICE falls within the range of 20000 and 70000.\n",
    "\n",
    "DAYS_BIRTH_CLASS: This binary feature flags applicants with a DAYS_BIRTH value less than -15000, indicating older individuals.\n",
    "\n",
    "CREDIT_UNDER_100K: This binary feature identifies applicants with an AMT_CREDIT value below 100,000.\n",
    "\n",
    "YEARS_EMPLOYED_CLASS: This binary feature categorizes applicants based on their DAYS_EMPLOYED value, considering values less than 100,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e075bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "application_train['_INCOME_CLASS']=pd.qcut(application_train['AMT_INCOME_TOTAL'],3,labels=['Low_INCOME', 'Medium_INCOME', 'High_INCOME'])\n",
    "application_train['ANNUITY_CLASS']=pd.qcut(application_train['AMT_ANNUITY'],3,labels=['Low_ANNUITY', 'Medium_ANNUITY', 'High_ANNUITY'])\n",
    "application_train['AMT_GOODS_PRICE_0.2_0.7']=np.where((application_train['AMT_GOODS_PRICE']<= 70000) & (application_train['AMT_GOODS_PRICE']>=20000) ,1,0)\n",
    "application_train['DAYS_BIRTH_CLASS']=np.where(application_train['DAYS_BIRTH']<-15000,1,0)\n",
    "application_train['CREDIT_UNDER_100K']=np.where(application_train['AMT_CREDIT']<100000,1,0)\n",
    "application_train['YEARS_EMPLOYED_CLASS']=np.where(application_train['DAYS_EMPLOYED']<100000,1,0)\n",
    "\n",
    "application_test['_INCOME_CLASS']=pd.qcut(application_test['AMT_INCOME_TOTAL'],3,labels=['Low_INCOME', 'Medium_INCOME', 'High_INCOME'])\n",
    "application_test['ANNUITY_CLASS']=pd.qcut(application_test['AMT_ANNUITY'],3,labels=['Low_ANNUITY', 'Medium_ANNUITY', 'High_ANNUITY'])\n",
    "application_test['AMT_GOODS_PRICE_0.2_0.7']=np.where((application_test['AMT_GOODS_PRICE']<= 7000) & (application_test['AMT_GOODS_PRICE']>=2000) ,1,0)\n",
    "application_test['DAYS_BIRTH_CLASS']=np.where(application_test['DAYS_BIRTH']<-15000,1,0)\n",
    "application_test['CREDIT_UNDER_100K']=np.where(application_test['AMT_CREDIT']<100000,1,0)\n",
    "application_test['YEARS_EMPLOYED_CLASS']=np.where(application_test['DAYS_EMPLOYED']<100000,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9889ac26",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_categorical_features = ['_INCOME_CLASS','YEARS_EMPLOYED_CLASS','ANNUITY_CLASS',\n",
    "                            'AMT_GOODS_PRICE_0.2_0.7','DAYS_BIRTH_CLASS','CREDIT_UNDER_100K']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586aff27",
   "metadata": {},
   "source": [
    "Let's test the statistical significance of the relationship between the new categorical features and the target variable, providing insights into their potential importance in predicting the target variable of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e94e6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2_test(application_train[new_categorical_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9beff81c",
   "metadata": {},
   "source": [
    "Based on the chi-squared test conducted on the new categorical features, the results indicate that several of these features exhibit a significant association with the target variable. The DAYS_BIRTH_CLASS, ANNUITY_CLASS, _INCOME_CLASS,AMT_GOODS_PRICE_0.2_0.7 and CREDIT_UNDER_100K features show statistically significant relationships with the target variable, as evidenced by their low p-values. This suggests that these features may be informative in predicting the target variable and can potentially be valuable in the modeling process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff8ab82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect highly missing numeric features \n",
    "application_train_numeric= application_train.select_dtypes(include=['number'])\n",
    "missings=application_train_numeric.loc[:,application_train_numeric.isna().mean() >= 0.6]\n",
    "highly_missing_features = missings.columns\n",
    "missings.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb77a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove highly missing features\n",
    "application_train.drop(highly_missing_features,axis=1,inplace=True)\n",
    "application_test.drop(highly_missing_features,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852a2123",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = application_train['TARGET']\n",
    "features_train = application_train.drop('TARGET', axis=1)\n",
    "\n",
    "# Impute numerical features using mean imputation\n",
    "numeric_imputer = SimpleImputer(strategy='median')\n",
    "numeric_features_train = features_train.select_dtypes(include='number')\n",
    "imputed_numeric_features_train = numeric_imputer.fit_transform(numeric_features_train)\n",
    "numeric_features_test = application_test.select_dtypes(include='number')\n",
    "imputed_numeric_features_test = numeric_imputer.transform(numeric_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23935cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features_train = features_train.select_dtypes(include='object')\n",
    "# using dummy_na=False ,we perform onehot encoding without affecting data distribution\n",
    "dummy_categorical_features_train = pd.get_dummies(categorical_features_train,dummy_na=False)\n",
    "\n",
    "# Concatenate imputed numerical and categorical features Train\n",
    "imputed_features_train = pd.concat([pd.DataFrame(imputed_numeric_features_train, columns=numeric_features_train.columns),\n",
    "                              dummy_categorical_features_train] ,axis=1)\n",
    "\n",
    "# Impute categorical features Test using mode imputation and one-hot encoding\n",
    "\n",
    "categorical_features_test = application_test.select_dtypes(include='object')\n",
    "dummy_categorical_features_test = pd.get_dummies(categorical_features_test,dummy_na=False)\n",
    "\n",
    "# Concatenate imputed numerical and categorical features\n",
    "imputed_features_test = pd.concat([pd.DataFrame(imputed_numeric_features_test, columns=numeric_features_test.columns),\n",
    "                              dummy_categorical_features_test],\n",
    "                             axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131d6ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_features_train.drop('NAME_INCOME_TYPE_Maternity leave',axis=1,inplace=True)\n",
    "imputed_features_train.drop('NAME_FAMILY_STATUS_Unknown',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8688b335",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=imputed_features_train.copy()\n",
    "y=target\n",
    "#splitting data to train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,stratify=y, test_size=0.25, random_state=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754697ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define class weights\n",
    "class_weights = {0: 1, 1: 9}\n",
    "\n",
    "simple_estimators = {'Logistic Regression':LogisticRegression(),\n",
    "                     'Decision Tree':DecisionTreeClassifier(),\n",
    "                     'Linear Discriminant Analysys': LinearDiscriminantAnalysis()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d1162b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_simple_models(estimators):\n",
    "    for name, model in estimators.items():\n",
    "        print(\"Running \" + name)\n",
    "\n",
    "        # Apply SMOTE oversampling to the training data\n",
    "        #smote = SMOTE()\n",
    "        #X_train_smt, y_train_smt = smote.fit_resample(X_train, y_train)\n",
    "        X_train_smt, y_train_smt = X_train, y_train\n",
    "        # Scaling data\n",
    "        scaler = MinMaxScaler()\n",
    "        X_train_scaled= scaler.fit_transform(X_train_smt)\n",
    "        X_test_scaled=scaler.transform(X_test)\n",
    "        \n",
    "        # Perform Recursive Feature Elimination (RFE)\n",
    "        selector = RFE(estimator=model)\n",
    "        X_train_selected = selector.fit_transform(X_train_scaled, y_train_smt)\n",
    "        X_test_selected = selector.transform(X_test_scaled)\n",
    "        \n",
    "        # Train the model on the selected features\n",
    "        model.fit(X_train_selected, y_train_smt)\n",
    "        \n",
    "        # Make predictions on the test data\n",
    "        y_pred = model.predict_proba(X_test_selected)[:, 1]\n",
    "        \n",
    "        # Calculate and print the ROC AUC score\n",
    "        roc_auc = roc_auc_score(y_test, y_pred)\n",
    "        print(\"roc_auc:\", roc_auc)\n",
    "        print('*' * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563b23cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_simple_models(simple_estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4fa6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the GradientBoostingClassifier class\n",
    "clf = GradientBoostingClassifier(n_estimators=100, subsample=0.2, random_state=42, verbose = 1)\n",
    "\n",
    "# Fit the model to the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = clf.predict_proba(X_test)[:,1]\n",
    "roc_GBC=roc_auc_score(y_test,y_pred)\n",
    "print(roc_GBC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e163c799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running Xgboost\n",
    "# Define class weights\n",
    "class_weights = {0: 1, 1: 9}\n",
    "\n",
    "# Create an instance of the XGBClassifier class\n",
    "xgb = XGBClassifier(n_estimators=100, subsample=0.4, random_state=1, scale_pos_weight=class_weights[1]/class_weights[0])\n",
    "\n",
    "# Fit the model to the training data\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = xgb.predict_proba(X_test)[:,1]\n",
    "roc_xgb=roc_auc_score(y_test,y_pred)\n",
    "print(roc_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58be32ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running Random Forest \n",
    "rfc_100 = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "rfc_100.fit(X_train, y_train)\n",
    "y_pred_100 = rfc_100.predict_proba(X_test)[:,1]\n",
    "print('Model auc score with 100 decision-trees : {0:0.4f}'. format(roc_auc_score(y_test, y_pred_100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7783127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning Gradient Boosting Classifier\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 100, 200, 400],\n",
    "    'max_depth': [10, 20, 30, 40, 50, 60],\n",
    "    'min_samples_split': [5,10, 20, 30],\n",
    "    'min_samples_leaf':[10, 20, 30, 40, 50, 60],\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'subsample': [0.8, 0.9, 1.0],\n",
    "    'learning_rate': [0.1, 0.01, 0.001]\n",
    "}\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=1, random_state=1)\n",
    "gbc = RandomizedSearchCV(GradientBoostingClassifier(),\n",
    "                            param_distributions=param_dist,\n",
    "                            n_iter=10,\n",
    "                            cv=cv,\n",
    "                            scoring='roc_auc',\n",
    "                            random_state=1,\n",
    "                            verbose=True,\n",
    "                            n_jobs=cpu_count())\n",
    "\n",
    "# Handle class imbalance by setting sample weights\n",
    "sample_weights = [1 if label == 0 else 9 for label in y_train]\n",
    "\n",
    "gbc.fit(X_train, y_train, sample_weight=sample_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c631e5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_gbc\n",
    "print(\"Best parameter (CV score=%0.3f):\" % gbc.best_score_)\n",
    "print(gbc.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d325306f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_curve__confusion_matrix(model):\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    y_prob_test = model.predict_proba(X_test)[:, 1]\n",
    "    y_prob_train  = model.predict_proba(X_train)[:, 1]\n",
    "    prediction=model.predict(X_test)\n",
    "    # calculate tpr ,fpr\n",
    "    fpr_test , tpr_test , thresholds = roc_curve(y_test, y_prob_test)\n",
    "    fpr_train , tpr_train , thresholds1 = roc_curve(y_train, y_prob_train)\n",
    "\n",
    "    # calculate AUC score\n",
    "    roc_auc_test  = auc(fpr_test, tpr_test)\n",
    "    roc_auc_train  = auc(fpr_train ,tpr_train)\n",
    "\n",
    "\n",
    "    # calculate confusion matrix for test set\n",
    "    confusion_matrix_test = confusion_matrix(y_test, prediction)\n",
    "\n",
    "    # create subplots\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    # plot ROC curves on the first subplot\n",
    "    axs[0].plot(fpr_train, tpr_train, color='green', label='ROC curve train (AUC = %0.2f)' % roc_auc_train)\n",
    "    axs[0].plot(fpr_test, tpr_test, color='blue', label='ROC curve test (AUC = %0.2f)' % roc_auc_test)\n",
    "    axs[0].plot([0, 1], [0, 1], color='red', linestyle='--')\n",
    "    axs[0].set_xlabel('False Positive Rate')\n",
    "    axs[0].set_ylabel('True Positive Rate')\n",
    "    axs[0].set_title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    axs[0].legend(loc=\"lower right\")\n",
    "\n",
    "    sns.set(font_scale=1.4)\n",
    "    sns.heatmap(confusion_matrix_test, annot=True, fmt='g', cmap='Blues', ax=axs[1])\n",
    "    axs[1].set_xlabel('Predicted label')\n",
    "    axs[1].set_ylabel('True label')\n",
    "    axs[1].set_title('Confusion Matrix (Test Set)model');\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a5d3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_curve__confusion_matrix(best_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58e2d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_curve__confusion_matrix(best_gbc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a09fd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance(model):  \n",
    "    \n",
    "    # Extract features importance from model\n",
    "    importances = model.feature_importances_\n",
    "    sorted_idx = importances.argsort()[::-1]\n",
    "    feature_scores = pd.Series(importances, index=X_train.columns).sort_values(ascending=False)\n",
    "    important_features=feature_scores[:10]\n",
    "    \n",
    "    # Creating a seaborn bar plot for features importance\n",
    "    f, ax = plt.subplots(figsize=(30, 24))\n",
    "    ax = sns.barplot(x=feature_scores, y=feature_scores.index, data=feature_scores)\n",
    "    ax.set_title(\"Visualize feature scores of the features\")\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "    ax.set_yticklabels(feature_scores.index)\n",
    "    ax.set_xlabel(\"Feature importance score\")\n",
    "    ax.set_ylabel(\"Features\")\n",
    "    plt.show()\n",
    "    print(important_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9888aeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(best_gbc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113b2626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a submission for Logistic Regression\n",
    "submit = application_test[['SK_ID_CURR']]\n",
    "submit['TARGET'] = best_LR.predict_proba(imputed_features_test)[:,1]\n",
    "submit.to_csv('submission_LR.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb67309",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b5bdc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
